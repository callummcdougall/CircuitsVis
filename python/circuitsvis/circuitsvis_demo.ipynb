{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mdCsqhXBg-IF"},"source":["# CircuitsVis: extensions\n","\n","This notebook contains my fork of circuitsvis. It includes a few extra features relative to the standard version, such as:\n","\n","* It can be run on a TransformerLens `ActivationCache` rather than just on a tensor of attention patterns (all examples in the notebook do this),\n","* Option to show value-weighted attention patterns rather than regular patterns,\n","* It can toggle between multiple sequences in a batch,\n","* Plots can be opened in browser by default rather than displayed inline (won't work in Colab),\n","* Added [bertviz](https://github.com/jessevig/bertviz)-style plots, with some extra features.\n","\n","Please comment or send me a message if you have any feedback. I hope you find this useful!\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/CallumMcDougall_circuit_visualisations_colorful_clear_simple_bo_6c1b2148-84e2-4ab8-906e-d104cdc85e06.png\" width=\"320\">"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_cQSNp3KZ5w6"},"source":["# Setup code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mhat43AVZ5w9"},"outputs":[],"source":["try:\n","    import google.colab\n","    %pip install transformer_lens\n","    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n","except:\n","    import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n","    from IPython import get_ipython\n","    ipython = get_ipython()\n","    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","    ipython.run_line_magic(\"autoreload\", \"2\")\n","\n","import torch as t\n","from torch import Tensor\n","import circuitsvis as cv\n","from jaxtyping import Float\n","from IPython.display import clear_output, display\n","from transformer_lens import HookedTransformer\n","\n","t.set_grad_enabled(False)\n","\n","gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\")\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51uwwxCdZ5w-"},"outputs":[],"source":["sentence0 = \"When Mary and John went to the shops, John gave a drink to Mary.\"\n","sentence1 = \"When Mary and John went to the shops, Mary gave a drink to John.\"\n","sentence2 = \"The cat sat on the mat.\"\n","sentences_all = [sentence0, sentence1, sentence2]\n","\n","names_filter = lambda name: any(name.endswith(f\"hook_{s}\") for s in [\"pattern\", \"q\", \"k\", \"v\"])\n","logits, cache = gpt2.run_with_cache(sentence0, names_filter=names_filter, remove_batch_dim=True)\n","logits_all, cache_all = gpt2.run_with_cache(sentences_all, names_filter=names_filter)\n","logits, cache_full = gpt2.run_with_cache(sentence0, remove_batch_dim=True)\n","\n","attn: Float[Tensor, \"layers heads seq_Q seq_K\"] = t.stack([cache[\"pattern\", i] for i in range(gpt2.cfg.n_layers)])\n","attn_all: Float[Tensor, \"batch layers heads seq_Q seq_K\"] = t.stack([cache_all[\"pattern\", i] for i in range(gpt2.cfg.n_layers)], dim=1)\n","\n","tokens = gpt2.to_str_tokens(sentence0)\n","tokens_all = gpt2.to_str_tokens(sentences_all)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wR1Wx-DmZ5w-"},"source":["# Overview of function\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wIu64WFJbU48"},"source":["The main function is `cv.attention.from_cache`. The most important arguments are:\n","\n","* `cache` - the `ActivationCache` object. This has to contain the appropriate activations (i.e. `pattern`, plus `v` if you're using value-weighted attention, plus `q` and `k` if you're using `lines` mode).\n","\n","* `tokens` - either a list of strings (if batch size is 1), or a list of lists of strings (if batch size is > 1)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yUzcS2VRbWHz"},"source":["The optional arguments are:\n","\n","* `heads` - if specified (e.g. `[(9, 6), (9, 9)]`), these heads will be shown in the visualisation. If not specified, behaviour is determined by the `layers` argument.\n","* `layers` - this can be an int (= single layer), list of ints (= list of layers), or None (= all layers). If `heads` are not specified, then the value of this argument determines what heads are shown.\n","* `batch_idx` - if the cache has a batch dimension, then you can specify this argument (as either an int, or list of ints). Note that you can have nontrivial batch size in your visualisations (you'll be able to select different sequences using a dropdown).\n","* `attention_type` - if this is `\"standard\"`, we just use raw attention patterns. If this is `\"value-weighted\"`, then the visualisation will use value-weighted attention, i.e. every attention probability $A^h[s_Q, s_K]$ will be replaced with:\n","\n","    $$\n","    A^h[s_Q, s_K] \\times \\frac{\\|v^h[s_K]\\|}{\\underset{s}{\\max} \\|v^h[s]\\|}\n","    $$\n","    \n","    If this is `\"info-weighted\"`, we get the same, except with each $v^h[s]$ replaced with $v^h[s]^T W_O^h$ (the output projection matrix for head $h$).\n","    \n","* `mode` - this can be \"large\", \"small\" or \"lines\", for producing the three different types of attention plots (see below for examples of all).\n","* `return_mode` - this can be \"browser\" (open plot in browser; doesn't work in Colab or VMs), \"html\" (returns html object), or \"view\" (displays object inline).\n","* `radioitems` - if True, you select the sequence in the batch using radioitems rather than a dropdown. Defaults to False.\n","* `batch_labels` - if you're using batch size > 1, then this argument can override `tokens` to be the thing you see in the dropdown / radioitems.\n","* `title` - if given, then a title is appended to the start of your plot (i.e. an `<h1>` HTML item).\n","* `head_notation` - can be either `\"dot\"` for notation like `10.7` (this is default), or `\"LH\"` for notation like `L10H7`.\n","* `help` - if True, prints out a string explaining the visualisation. Defulats to False."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gNmjzhjEZ5w_"},"source":["# Examples\n","\n","Below is a set of examples, along with some brief explanations."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L4qafzzLjvYW"},"source":["## Visualising batches"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VO54Lj-TZ5w_"},"source":["The 4 examples below illustrate how you can batch circuitsvis figures together. They are:\n","\n","1. Cache with no batch dim (this works like normal circuitsvis)\n","2. Cache with batch dim (there's an extra dropdown where you can choose different sequences in the batch)\n","3. Cache with batch dim, but with `batch_idx` specified as an int - this causes behaviour like (1)\n","4. Cache with batch dim, but with `batch_idx` specified as a list - this causes behaviour like (2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"elapsed":1144,"status":"ok","timestamp":1688585737827,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"93SNPRF8Z5w_","outputId":"28dfe18f-6e39-4a5d-c70a-4804a4c11b3c"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    layers = 0,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":791},"executionInfo":{"elapsed":1870,"status":"ok","timestamp":1688585739695,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"ewk44u5LZ5w_","outputId":"d19e1326-180d-4375-c96a-bfe29e880a59"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    layers = 0,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1688585739695,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"RVx4dL1xZ5xA","outputId":"1be20c5f-f48e-474b-a03b-115e0c118ab4"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    layers = 0,\n","    batch_idx = 1, # Different way to specify a sequence within batch\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    layers = 0,\n","    batch_idx = [0, 1], # Different way to specify some sequences within batch\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OWAlW_14Z5xA"},"source":["## Specifying layers and heads"]},{"cell_type":"markdown","metadata":{},"source":["\n","You saw above how we can specify layers using the `layers` argument. You can also use the `heads` argument to specify given heads. The full options are:\n","\n","* When both are `None`, all layers and heads are shown.\n","* When `layers` is given (as an int or a list of ints) but `heads` is None, all heads in the given layer(s) are shown.\n","* When `layers` is None, and `heads` is given (as a tuple of `(layer, head_idx)` or a list of such tuples), only the given heads are shown.\n","\n","We have a couple of examples below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"xND4kBmuZ5xA","outputId":"534b08c7-bc4e-483d-8c2f-3a9eefa376ab"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    layers = [0, -1], # Negative indices are accepted\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"QntJRlEyZ5xB","outputId":"4976bc3f-f194-437f-bdea-d79add3b577e"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)], # Showing all the name mover heads: `to` attends to the IO token `Mary`\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4g7LcILIjpHG"},"source":["## Different modes: `\"large\"` and `\"lines\"`\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fKKk3lAaZ5xB"},"source":["\n","The mode above is \"small\" (also known as `attention_patterns` in circuitsvis). You can also use \"large\" (which is `attention_heads` in circuitsvis), or \"lines\" (which is like the neuron view in [bertviz](https://github.com/jessevig/bertviz), but with a few extra features I added, e.g. showing values of the attention scores as well as probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":864},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"LbBZEzyrZ5xB","outputId":"a2991ded-7bee-4c76-ee3d-5ebd2299d683"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"large\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"output_embedded_package_id":"169DOZJxlteO0SgFGwAbTqi8DfM5Gq3mX"},"executionInfo":{"elapsed":7768,"status":"ok","timestamp":1688585747455,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"HQC1NbOJZ5xB","outputId":"a2d2f560-306b-46e5-e276-629e488fb9e2"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    mode = \"lines\",\n","    display_mode = \"dark\", # Can also choose \"light\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"output_embedded_package_id":"1LLFhRGNx_4OObMsRNr-Q7UjDcDbNBsiC"},"executionInfo":{"elapsed":8725,"status":"ok","timestamp":1688585756177,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"B5jFwh5JZ5xB","outputId":"46a8974c-18b1-47ac-cfd2-33c1a2507656"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"lines\",\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2OZzBEuxjnBz"},"source":["## Value-weighted attention\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j8uFtMYhZ5xB"},"source":["Value-weighted attention is a pretty neat concept. TL;DR - **if a value vector doubled in magnitude but attention probability halved then nothing would change; so we should expect the attention probability to be more meaningful once we scale by the magnitude of the value vector**. This is what the `attention_type` argument does. If it is `\"value-weighted\"`, then every attention probability $A^h[s_Q, s_K]$ will be replaced with:\n","\n","$$\n","A^h[s_Q, s_K] \\times \\frac{\\|v^h[s_K]\\|}{\\underset{s}{\\max} \\|v^h[s]\\|}\n","$$\n","\n","where $\\|v^h[s]\\|$ is the $L_2$ norm of the value vector at source position $s$ in head $h$.\n","\n","If it is `\"info-weighted\"`, then instead we get:\n","\n","$$\n","A^h[s_Q, s_K] \\times \\frac{\\Big\\|v^h[s_K]^T W_O^h\\Big\\|}{\\underset{s}{\\max} \\Big\\|v^h[s]^T W_O^h\\Big\\|}\n","$$\n","\n","In particular, when we do this, we can see that the attention on the BOS token is much lower (because usually this token is attended to as a placeholder, and not much is actually copied)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688585756177,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"UDGPt4QXZ5xB","outputId":"17eb4081-263f-4964-9394-ba10ab340a02"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)], # showing all the name mover heads: `to` attends to the IO token `Mary`\n","    attention_type = \"info-weighted\", # or try \"value-weighted\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TBGtYCkgjlC5"},"source":["## Other arguments"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UrpKtSOwZ5xC"},"source":["`title` (optional) specifies a title.\n","\n","If `help` is True, then a string explaining the visualisation is printed out (as well as an explanation of the non-default arguments which you're using).\n","\n","If `radioitems` is True, then you select different sequences in the batch using radioitems rather than a dropdown. Defaults to False.\n","\n","If `batch_labels` is not None (and your batch size is larger than 1), then this argument overrides the values that appear in the dropdown / radioitems. It can either be a list of strings, or a function mapping `(batch_idx, tokens[batch_idx])` to a string.\n","\n","`head_notation` can be set to `\"LH\"` to change the notation from e.g. `10.7` to `L10H7`.\n","\n","`display_mode` can be \"dark\" (default) or \"light\". This only affects the \"lines\" mode.\n","\n","`return_mode` can be \"browser\" (open in browser), \"html\" (return html object), or \"view\" (display object inline; this is default).\n","\n","Note that the browser view is often preferable - it doesn't slow down your IDE, and it can reduce flickering when you switch between different sequences in your batch. However, this won't always work (e.g. in virtual machines or on Colab). In this case, you should use `return_mode = \"html\"`, then save the result and download & open it manually.\n","\n","I've given 2 examples below, which both showcase a several of these features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":758,"output_embedded_package_id":"17d-e_wInpsJUQelUQS2uzEXC5q-Hii6D"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688585756178,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"HYuN8VpMZ5xC","outputId":"40734aca-a9be-4083-9239-72b1af679121"},"outputs":[],"source":["html_object = cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    batch_idx = 0,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"lines\",\n","    title = \"Attention of name mover heads (lines mode)\",\n","    return_mode = \"html\",\n","    help = True,\n","    display_mode = \"light\", # This might be better if you're opening in browser\n",")\n","\n","display(html_object)\n","\n","with open(\"my_file.html\", \"w\") as f:\n","    f.write(html_object.data)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<!DOCTYPE html>\n","<html>\n","<head>\n","    <title>HTML Dropdown</title>\n","</head>\n","<body>\n","\n","    <form>\n","        <label>Choose a sequence:</label>\n","        <div>\n","            <input type=\"radio\" id=\"set1\" name=\"tokens\" value=\"set1\" onclick=\"changeTokens(this.value)\">\n","            <label for=\"set1\"><code>|When| Mary| and| John| went| to| the| shops|,| John| gave| a| drink| to| Mary|.</code></label>\n","        </div>\n","        <div>\n","            <input type=\"radio\" id=\"set2\" name=\"tokens\" value=\"set2\" onclick=\"changeTokens(this.value)\">\n","            <label for=\"set2\"><code>|When| Mary| and| John| went| to| the| shops|,| Mary| gave| a| drink| to| John|.</code></label>\n","        </div>\n","        <div>\n","            <input type=\"radio\" id=\"set3\" name=\"tokens\" value=\"set3\" onclick=\"changeTokens(this.value)\">\n","            <label for=\"set3\"><code>|The| cat| sat| on| the| mat|.</code></label>\n","        </div>\n","    </form>\n","\n","    <div id=\"circuits-vis-7252ed2c-7408\" style=\"margin: 15px 0;\"></div>\n","    \n","    <script crossorigin type=\"module\">\n","        import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n","        \n","        let tokens = {\n","            \"set1\": ['', 'When', ' Mary', ' and', ' John', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' a', ' drink', ' to', ' Mary', '.'],\n","            \"set2\": ['', 'When', ' Mary', ' and', ' John', ' went', ' to', ' the', ' shops', ',', ' Mary', ' gave', ' a', ' drink', ' to', ' John', '.'],\n","            \"set3\": ['', 'The', ' cat', ' sat', ' on', ' the', ' mat', '.'],\n","        };\n","        let attention = {\n","            \"set1\": [[[0.07940362393856049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07928484678268433, 0.0009351870976388454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07915613055229187, 0.00022118557535577565, 0.0027630680706351995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07138746231794357, 0.000476348475785926, 0.09727059304714203, 0.0016504693776369095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07646659016609192, 0.0005180498119443655, 0.00606760336086154, 0.010419302619993687, 0.007554819341748953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07893640547990799, 0.0005108417826704681, 0.0014636063715443015, 0.00022231650655157864, 0.0002869920281227678, 0.002200051210820675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0780416950583458, 0.0009758734959177673, 0.00782921351492405, 0.00030290771974250674, 0.0022584102116525173, 0.0009491113014519215, 0.0018170176772400737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07901634275913239, 0.0006175601738505065, 0.0009868140332400799, 0.00029688343056477606, 0.0001826882507884875, 0.0003776824742089957, 0.0005418286891654134, 0.0005353175802156329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07769576460123062, 0.005369691178202629, 0.0011959864059463143, 0.0003053593391086906, 0.0004622149281203747, 0.0002769508573692292, 0.001255597802810371, 0.0005516299279406667, 0.005345144309103489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05106537789106369, 0.00518394261598587, 0.1945294886827469, 0.0022033678833395243, 0.09088976681232452, 0.0008534168591722846, 0.00040015229023993015, 0.0003317017399240285, 0.004610053729265928, 0.0009057212737388909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07558515667915344, 0.0013451382983475924, 0.004230285994708538, 0.009658790193498135, 0.004061327315866947, 0.005856170319020748, 0.001313112094067037, 0.00030178017914295197, 0.004013719502836466, 0.000678564771078527, 0.0012550121173262596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03571956232190132, 0.002001452259719372, 0.37931928038597107, 0.004367887508124113, 0.07142563164234161, 0.0004746418271679431, 0.00029279140289872885, 0.0008900616085156798, 0.002194127533584833, 0.00023664456966798753, 0.027174463495612144, 0.000979805365204811, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07740961015224457, 0.0007642305572517216, 0.0016240256372839212, 0.001425577444024384, 0.00041269921348430216, 0.0005049321916885674, 0.0005187205388210714, 0.00016328324272762984, 0.0044039105996489525, 0.00036284225643612444, 0.00024912095977924764, 0.0010140315862372518, 0.0056197913363575935, 0.0, 0.0, 0.0, 0.0], [0.07753273099660873, 0.0014450494199991226, 0.0010603427654132247, 0.00022962410002946854, 0.0005349291022866964, 0.00014170832582749426, 0.0004992972826585174, 0.00011317867756588385, 0.0013316577533259988, 0.0011637363350018859, 0.0005554014351218939, 0.000539178668987006, 0.0030549231451004744, 0.006540827918797731, 0.0, 0.0, 0.0], [0.018143439665436745, 0.0009564865031279624, 0.6217044591903687, 0.0007556757191196084, 0.07171507179737091, 5.488670285558328e-05, 1.8546328647062182e-05, 9.740063251229003e-05, 0.00041534469346515834, 3.518270023050718e-05, 0.022071121260523796, 4.353499389253557e-05, 0.00024992116959765553, 0.0002949122281279415, 0.00020560827397275716, 0.0, 0.0], [0.0750759169459343, 0.0011943011777475476, 0.0046477667056024075, 0.00614228006452322, 0.012595288455486298, 0.0009198134648613632, 0.0006324183777906001, 9.558202873449773e-05, 0.0033210439141839743, 0.0009790138574317098, 0.00145335856359452, 0.0010007201926782727, 0.0003082846524193883, 0.001478391233831644, 0.0008193319081328809, 0.0014476226642727852, 0.0], [0.01866450160741806, 0.0034736301749944687, 0.4685545563697815, 0.0008656050777062774, 0.12118032574653625, 0.0001909978163894266, 7.134003681130707e-05, 0.00015293629257939756, 0.0009189056581817567, 9.217580372933298e-05, 0.037400297820568085, 0.0002655193966347724, 0.0005812453455291688, 0.0024397801607847214, 0.0002990960201714188, 0.030338730663061142, 0.0007931421278044581]], [[0.04721849039196968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04712076857686043, 0.001386370393447578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04704708978533745, 0.00029217457631602883, 0.003193726297467947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04449182003736496, 0.0014134381199255586, 0.0511593334376812, 0.0022739674896001816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046613167971372604, 0.0010145646519958973, 0.0023116539232432842, 0.001342944335192442, 0.004815078806132078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04703329876065254, 0.0006435569957830012, 0.0002218688459834084, 9.85477163339965e-05, 0.00034514113212935627, 0.0013817015569657087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04621994122862816, 0.0012693695025518537, 0.0032670386135578156, 0.00022417854052037, 0.006144155282527208, 0.0009768134914338589, 0.003255152143537998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0469621866941452, 0.0008854191983118653, 0.0006533428677357733, 0.00014597507833968848, 0.0003423892776481807, 0.00011611913942033425, 0.00019595579942688346, 0.0013421684270724654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04679431393742561, 0.0009297600481659174, 0.0009249914437532425, 0.00019397330470383167, 0.00023484069970436394, 0.0001250639179488644, 0.00023797646281309426, 0.00017170869978144765, 0.002558624604716897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02728809230029583, 0.005483135115355253, 0.20603244006633759, 0.0022333976812660694, 0.14318780601024628, 0.0006234131869859993, 0.0005946726305410266, 0.0016492283903062344, 0.0029038977809250355, 0.0022392896935343742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04518318921327591, 0.0025157711934298277, 0.0026959930546581745, 0.002519813133403659, 0.006879840511828661, 0.008105113171041012, 0.0007480485364794731, 0.0004293486999813467, 0.002313970820978284, 0.0009552249684929848, 0.001417770516127348, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020626479759812355, 0.013201389461755753, 0.41332322359085083, 0.0019641797989606857, 0.042160894721746445, 0.0004809963866136968, 0.00033320661168545485, 0.0014417279744520783, 0.003387077944353223, 0.0016961225774139166, 0.03950276970863342, 0.002519281581044197, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04628067836165428, 0.0008599032298661768, 0.0022167188581079245, 0.0008314056904055178, 0.0005067176534794271, 0.00020482575928326696, 0.00018590975378174335, 0.0001328071957686916, 0.003333916189149022, 0.0003704110567923635, 0.00021357095101848245, 0.0012084810296073556, 0.0024343524128198624, 0.0, 0.0, 0.0, 0.0], [0.046090926975011826, 0.0014222944155335426, 0.0006986433290876448, 0.00023903461988084018, 0.0004309005453251302, 7.636600639671087e-05, 0.0005008861189708114, 0.00011428143625380471, 0.00172063906211406, 0.0009376124362461269, 0.00023130280897021294, 0.0009800735861063004, 0.002203023061156273, 0.004055604338645935, 0.0, 0.0, 0.0], [0.011789251118898392, 0.010173939168453217, 0.5675994753837585, 0.0012216346804052591, 0.059706710278987885, 5.134798993822187e-05, 4.9356814997736365e-05, 0.0006777968956157565, 0.00033134431578218937, 0.0010010432451963425, 0.05837295204401016, 0.000136211805511266, 0.0013745707692578435, 0.00031135461176745594, 0.0003223458770662546, 0.0, 0.0], [0.03810175135731697, 0.001506855129264295, 0.0035661025904119015, 0.055072102695703506, 0.0495002344250679, 0.00040833267848938704, 0.00042573249083943665, 0.0007371495594270527, 0.0007634153007529676, 0.0007893450674600899, 0.0028325291350483894, 0.00019396963762119412, 0.00012065941700711846, 0.0005379424546845257, 0.0004763425968121737, 0.0013350923545658588, 0.0], [0.014158866368234158, 0.010359820909798145, 0.3785182237625122, 0.0017829561838880181, 0.124483123421669, 0.00022695749066770077, 0.00020439988293219358, 0.0009092550608329475, 0.0010190291795879602, 0.0006957765435799956, 0.05849308893084526, 0.00044673739466816187, 0.0024971801321953535, 0.0021414661314338446, 0.0005632018437609076, 0.02341458573937416, 0.010756107047200203]], [[0.053441938012838364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.052248865365982056, 0.01287266705185175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05075867474079132, 0.006635752506554127, 0.035271305590867996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04599473625421524, 0.0028849353548139334, 0.11727334558963776, 0.003322251606732607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046863436698913574, 0.00867824349552393, 0.04484628513455391, 0.008955461904406548, 0.028774481266736984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05167321488261223, 0.002352524548768997, 0.007956045679748058, 0.0030246113892644644, 0.002198738045990467, 0.009161991998553276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03972601518034935, 0.010490549728274345, 0.09369223564863205, 0.0068504163064062595, 0.04723702743649483, 0.01539862621575594, 0.01788835972547531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04732195660471916, 0.005542371422052383, 0.027414962649345398, 0.0060952394269406796, 0.013822774402797222, 0.005737704690545797, 0.011715703643858433, 0.006960356142371893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04965541511774063, 0.002772208070382476, 0.027782687917351723, 0.0007511239382438362, 0.009015144780278206, 0.0010867648525163531, 0.0005076320376247168, 0.00038776255678385496, 0.013313519768416882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024008797481656075, 0.0027986972127109766, 0.25844278931617737, 0.004091609735041857, 0.134001225233078, 0.002786994678899646, 0.0014728348469361663, 0.001354025211185217, 0.026550747454166412, 0.003716463455930352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04372360184788704, 0.005577741656452417, 0.018345806747674942, 0.00640967208892107, 0.014001737348735332, 0.012611516751348972, 0.002665653359144926, 0.0012620113557204604, 0.04020341485738754, 0.006682932376861572, 0.0196316447108984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0144504364579916, 0.0025345906615257263, 0.3489324450492859, 0.004392645321786404, 0.12956543266773224, 0.002093031071126461, 0.002172113861888647, 0.0017995013622567058, 0.012630912475287914, 0.0023630214855074883, 0.07082635909318924, 0.003430963261052966, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040932632982730865, 0.0021138412412256002, 0.011457053944468498, 0.010187901556491852, 0.004049292299896479, 0.0033677108585834503, 0.004282783716917038, 0.0016944105736911297, 0.10425553470849991, 0.0029847491532564163, 0.0032993515487760305, 0.015375660732388496, 0.004322314169257879, 0.0, 0.0, 0.0, 0.0], [0.048919714987277985, 0.000916563265491277, 0.0020737843587994576, 0.0005399070796556771, 0.00078769022366032, 0.00036624539643526077, 0.0006825575837865472, 0.0002492101339157671, 0.015763862058520317, 0.0010080448118969798, 0.001146346447058022, 0.0033395281061530113, 0.0016414287965744734, 0.04411951079964638, 0.0, 0.0, 0.0], [0.02261817641556263, 0.002708339598029852, 0.290666401386261, 0.0034490320831537247, 0.07410626113414764, 0.0018253371817991138, 0.0014931481564417481, 0.0011135932290926576, 0.015539543703198433, 0.0024875286035239697, 0.039982859045267105, 0.0018119363812729716, 0.002711328212171793, 0.023963458836078644, 0.006879124324768782, 0.0, 0.0], [0.038496389985084534, 0.004475183319300413, 0.018883073702454567, 0.004956280812621117, 0.023688914254307747, 0.005621562246233225, 0.001985910814255476, 0.0010461987694725394, 0.03445395082235336, 0.005857391282916069, 0.01412664633244276, 0.004652738105505705, 0.000957908108830452, 0.07004737854003906, 0.007992807775735855, 0.017756173387169838, 0.0], [0.018022947013378143, 0.002153222681954503, 0.3162112534046173, 0.0015401666751131415, 0.09444265067577362, 0.0007532304152846336, 0.0004650361661333591, 0.0003711491299327463, 0.004103353247046471, 0.0008577269036322832, 0.048196401447057724, 0.0018207257380709052, 0.0011725433869287372, 0.008828043937683105, 0.0025667408481240273, 0.05038578435778618, 0.0049674431793391705]]],\n","            \"set2\": [[[0.07940362393856049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07928484678268433, 0.0009351870976388454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07915613055229187, 0.00022118557535577565, 0.0027630680706351995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07138746231794357, 0.000476348475785926, 0.09727059304714203, 0.0016504693776369095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07646659016609192, 0.0005180498119443655, 0.00606760336086154, 0.010419302619993687, 0.007554819341748953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07893640547990799, 0.0005108417826704681, 0.0014636063715443015, 0.00022231650655157864, 0.0002869920281227678, 0.002200051210820675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0780416950583458, 0.0009758734959177673, 0.00782921351492405, 0.00030290771974250674, 0.0022584102116525173, 0.0009491113014519215, 0.0018170176772400737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07901634275913239, 0.0006175601738505065, 0.0009868140332400799, 0.00029688343056477606, 0.0001826882507884875, 0.0003776824742089957, 0.0005418286891654134, 0.0005353175802156329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07769576460123062, 0.005369691178202629, 0.0011959864059463143, 0.0003053593391086906, 0.0004622149281203747, 0.0002769508573692292, 0.001255597802810371, 0.0005516299279406667, 0.005345144309103489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05106537789106369, 0.00518394261598587, 0.1945294886827469, 0.0022033678833395243, 0.09088976681232452, 0.0008534168591722846, 0.00040015229023993015, 0.0003317017399240285, 0.004610053729265928, 0.0009057212737388909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07409101724624634, 0.000896437733899802, 0.004779188893735409, 0.019240112975239754, 0.005413435399532318, 0.005268935114145279, 0.0016382726607844234, 0.00021167530212551355, 0.0031282391864806414, 0.0005763437366113067, 0.003043894190341234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03698882460594177, 0.0010484250960871577, 0.13353805243968964, 0.006315795239061117, 0.22509878873825073, 0.0015073259128257632, 0.00036593482946045697, 0.0008138503762893379, 0.005023895297199488, 0.00039777441998012364, 0.019459109753370285, 0.003345593111589551, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07703504711389542, 0.0007187018636614084, 0.0014759537298232317, 0.001449263421818614, 0.0004570041492115706, 0.0005644244374707341, 0.0005434967461042106, 0.0001784389460226521, 0.005114493891596794, 0.00037181112566031516, 0.00046073750127106905, 0.0015775561332702637, 0.0072779348120093346, 0.0, 0.0, 0.0, 0.0], [0.0774221196770668, 0.0014807587722316384, 0.001033504493534565, 0.00024749551084823906, 0.0006277515203692019, 0.00016614372725598514, 0.0004875771119259298, 0.00011404481483623385, 0.0015315581113100052, 0.0012274497421458364, 0.0007067455444484949, 0.0004602701519615948, 0.0034599462524056435, 0.006777291651815176, 0.0, 0.0, 0.0], [0.028084030374884605, 0.0005018843803554773, 0.12594665586948395, 0.0013471344718709588, 0.3210975229740143, 0.00025517010362818837, 3.108039891230874e-05, 9.26213979255408e-05, 0.001267615589313209, 0.00014204082253854722, 0.01265079714357853, 0.0001864285150077194, 0.0009915337432175875, 0.0009713374893181026, 0.0007928520208224654, 0.0, 0.0], [0.07758927345275879, 0.0010862486669793725, 0.00175873888656497, 0.0018308068392798305, 0.004239863716065884, 0.0005208930815570056, 0.00019908070680685341, 5.402377792051993e-05, 0.0015690483851358294, 0.0008686238434165716, 0.0007085474207997322, 0.00019898040045518428, 0.0002456747170072049, 0.0011651342501863837, 0.00034711771877482533, 0.0009469162905588746, 0.0], [0.019978394731879234, 0.0024790745228528976, 0.24452592432498932, 0.0010719875572249293, 0.23132692277431488, 0.00030888390028849244, 8.35749669931829e-05, 0.00014203239697963, 0.001437328290194273, 0.00016174105985555798, 0.046864572912454605, 0.000228480581426993, 0.0007013770518824458, 0.003401106456294656, 0.00030629083630628884, 0.04733290895819664, 0.0011083452263846993]], [[0.04721849039196968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04712076857686043, 0.001386370393447578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04704708978533745, 0.00029217457631602883, 0.003193726297467947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04449182003736496, 0.0014134381199255586, 0.0511593334376812, 0.0022739674896001816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046613167971372604, 0.0010145646519958973, 0.0023116539232432842, 0.001342944335192442, 0.004815078806132078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04703329876065254, 0.0006435569957830012, 0.0002218688459834084, 9.85477163339965e-05, 0.00034514113212935627, 0.0013817015569657087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04621994122862816, 0.0012693695025518537, 0.0032670386135578156, 0.00022417854052037, 0.006144155282527208, 0.0009768134914338589, 0.003255152143537998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0469621866941452, 0.0008854191983118653, 0.0006533428677357733, 0.00014597507833968848, 0.0003423892776481807, 0.00011611913942033425, 0.00019595579942688346, 0.0013421684270724654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04679431393742561, 0.0009297600481659174, 0.0009249914437532425, 0.00019397330470383167, 0.00023484069970436394, 0.0001250639179488644, 0.00023797646281309426, 0.00017170869978144765, 0.002558624604716897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02728809230029583, 0.005483135115355253, 0.20603244006633759, 0.0022333976812660694, 0.14318780601024628, 0.0006234131869859993, 0.0005946726305410266, 0.0016492283903062344, 0.0029038977809250355, 0.0022392896935343742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03972300514578819, 0.0018678747583180666, 0.006263976916670799, 0.052936945110559464, 0.02761225216090679, 0.0017578222323209047, 0.00025409492081962526, 0.0005387800629250705, 0.0009008588385768235, 0.0005672106053680182, 0.001721048727631569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012720669619739056, 0.005072135012596846, 0.11132746934890747, 0.013445465825498104, 0.4155181050300598, 0.0010424829088151455, 0.000866700429469347, 0.0034820479340851307, 0.00437624566257, 0.0013776903506368399, 0.010505486279726028, 0.0025715238880366087, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046181727200746536, 0.000829346536193043, 0.0020578193943947554, 0.001487270463258028, 0.0007378635928034782, 0.0002265798975713551, 0.0002136404364136979, 0.00017351986025460064, 0.0033680626656860113, 0.0003827564651146531, 0.00016844028141349554, 0.001405011978931725, 0.002653401577845216, 0.0, 0.0, 0.0, 0.0], [0.04605396091938019, 0.0014724589418619871, 0.0006645183311775327, 0.00029892727616243064, 0.0005712354322895408, 7.928902778076008e-05, 0.0005539527628570795, 0.00012847482867073268, 0.0017447760328650475, 0.0009614803711883724, 9.715302439872175e-05, 0.0007197406957857311, 0.002085666870698333, 0.00456609483808279, 0.0, 0.0, 0.0], [0.005735803861171007, 0.0020278641022741795, 0.04645634442567825, 0.002722971374168992, 0.6135552525520325, 0.00011186925257788971, 0.0001343747862847522, 0.001065572490915656, 0.00043812324292957783, 0.0006375862867571414, 0.004404556471854448, 0.00012792120105586946, 0.001521032303571701, 0.00044094378245063126, 0.0005630923551507294, 0.0, 0.0], [0.04624835029244423, 0.0011711796978488564, 0.0005709801916964352, 0.001195860910229385, 0.005768525414168835, 0.001149338437244296, 0.0006055715493857861, 0.0002404999831924215, 0.0007024031947366893, 0.0005916043883189559, 0.00011908519081771374, 7.070263382047415e-05, 0.00022238990641199052, 0.00018651693244464695, 8.159463322954252e-05, 0.0009619659394957125, 0.0], [0.010696307756006718, 0.005534630734473467, 0.1672568917274475, 0.0015253204619511962, 0.30739620327949524, 0.0004485670942813158, 0.0003677041386254132, 0.0011696938890963793, 0.0016887352103367448, 0.0008138553821481764, 0.036135658621788025, 0.0002573349338490516, 0.0027018007822334766, 0.0025815411936491728, 0.0004771508392877877, 0.06701121479272842, 0.01938902586698532]], [[0.053996745496988297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0527912899851799, 0.013006304390728474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.051285628229379654, 0.006704641971737146, 0.03563747555017471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04647223278880119, 0.0029148852918297052, 0.11849082261323929, 0.003356741741299629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.047349948436021805, 0.008768335916101933, 0.045311857014894485, 0.009048433043062687, 0.029073204845190048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05220966041088104, 0.00237694731913507, 0.008038640953600407, 0.0030560113955289125, 0.002221564296633005, 0.009257107973098755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04013843089342117, 0.01059945672750473, 0.09466490149497986, 0.006921534426510334, 0.047727420926094055, 0.015558486804366112, 0.018074069172143936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04781322926282883, 0.005599909462034702, 0.02769957110285759, 0.006158517207950354, 0.013966275379061699, 0.005797271151095629, 0.01183733157813549, 0.007032615598291159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.050170909613370895, 0.0028009875677525997, 0.028071114793419838, 0.0007589217857457697, 0.00910873617976904, 0.0010980471270158887, 0.0005129021010361612, 0.00039178814040496945, 0.013451734557747841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024258045479655266, 0.0028277519159018993, 0.2611258327960968, 0.004134086892008781, 0.13539236783981323, 0.0028159278444945812, 0.0014881251845508814, 0.0013680820120498538, 0.026826385408639908, 0.0037550460547208786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04461916908621788, 0.004927292000502348, 0.01804603822529316, 0.008856612257659435, 0.01203225553035736, 0.012415877543389797, 0.0036230781115591526, 0.0013307783519849181, 0.03193335235118866, 0.00621726643294096, 0.026427287608385086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02160092070698738, 0.002639873418956995, 0.11251670867204666, 0.008230122737586498, 0.2430914342403412, 0.0019052890129387379, 0.0023171117063611746, 0.0013578617945313454, 0.021502861753106117, 0.002581195905804634, 0.03834252059459686, 0.004032739903777838, 0.0, 0.0, 0.0, 0.0, 0.0], [0.042047858238220215, 0.0018440951826050878, 0.008474885486066341, 0.008897410705685616, 0.0036868543829768896, 0.003288509789854288, 0.004148037172853947, 0.001464936649426818, 0.10530839115381241, 0.0027613146230578423, 0.004236683715134859, 0.014161963015794754, 0.0034104136284440756, 0.0, 0.0, 0.0, 0.0], [0.04996205121278763, 0.0009613570873625576, 0.0019487105309963226, 0.0005472821649163961, 0.0008316366584040225, 0.0003682028909679502, 0.000680775148794055, 0.0002312123979208991, 0.015369183383882046, 0.0009391626226715744, 0.0015965718775987625, 0.0023443286772817373, 0.001261144527234137, 0.03729848936200142, 0.0, 0.0, 0.0], [0.02709311433136463, 0.0028927470557391644, 0.06730598956346512, 0.006494556088000536, 0.2113417088985443, 0.0014882497489452362, 0.001285802572965622, 0.0007317422423511744, 0.02600380778312683, 0.0022282758727669716, 0.015532172285020351, 0.000829010852612555, 0.0013977695489302278, 0.018491830676794052, 0.0037112657446414232, 0.0, 0.0], [0.04293689876794815, 0.0032165134325623512, 0.0052885026670992374, 0.0022148319985717535, 0.012974313460290432, 0.0027762125246226788, 0.0007177236257120967, 0.0004325669433455914, 0.0278033334761858, 0.00359712028875947, 0.006138215307146311, 0.001539688790217042, 0.0005040924879722297, 0.06785247474908829, 0.0030212432611733675, 0.026877660304307938, 0.0], [0.02108917571604252, 0.002064055297523737, 0.19159379601478577, 0.0020341859199106693, 0.1330622285604477, 0.0006829024059697986, 0.0004053492157254368, 0.0003465030749794096, 0.0057577816769480705, 0.000788124103564769, 0.04926273599267006, 0.0008632849785499275, 0.0008395548211410642, 0.009625633247196674, 0.0015156834851950407, 0.07245375961065292, 0.005578939337283373]]],\n","            \"set3\": [[[0.0768880769610405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07674887031316757, 0.0007939001661725342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07527781277894974, 0.0014064746210351586, 0.01773553341627121, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07577336579561234, 0.0005361551884561777, 0.006649167276918888, 0.005723214708268642, 0.0, 0.0, 0.0, 0.0], [0.07525422424077988, 0.0016749424394220114, 0.013632148504257202, 0.0005977672408334911, 0.002213917439803481, 0.0, 0.0, 0.0], [0.07243992388248444, 0.0001805659121600911, 0.04649500548839569, 0.0017139832489192486, 0.0027480614371597767, 0.0031709468457847834, 0.0, 0.0], [0.07580283284187317, 0.000649352790787816, 0.003590869717299938, 0.0030501512810587883, 0.0022182052489370108, 0.0004183048731647432, 0.0012608777033165097, 0.0], [0.06994739919900894, 0.004945468157529831, 0.05402127653360367, 0.0025519737973809242, 0.001830288558267057, 0.0022500762715935707, 0.007551000919193029, 0.0031055121216923]], [[0.052774544805288315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05260055139660835, 0.0016869090031832457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05246639624238014, 0.0004032687284052372, 0.004773649387061596, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05264098942279816, 0.00013565734843723476, 0.0005029357271268964, 0.0015504546463489532, 0.0, 0.0, 0.0, 0.0], [0.05220307409763336, 0.0015015749959275126, 0.0035099026281386614, 0.0007196074002422392, 0.0022821580059826374, 0.0, 0.0, 0.0], [0.052165646106004715, 0.00031747607863508165, 0.006477953400462866, 0.0009342522826045752, 0.0006021099979989231, 0.0014266874641180038, 0.0, 0.0], [0.052299629896879196, 0.00032725027995184064, 0.0009188849944621325, 0.0013206199510022998, 0.0025981308426707983, 0.0007297684787772596, 0.0009791384218260646, 0.0], [0.048774875700473785, 0.005618053954094648, 0.030638184398412704, 0.004717549774795771, 0.0034115512389689684, 0.003105706535279751, 0.006010528188198805, 0.007857960648834705]], [[0.044372864067554474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0435965470969677, 0.005170519929379225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04144657403230667, 0.002166491700336337, 0.05861705541610718, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04182087257504463, 0.0007037754985503852, 0.028315763920545578, 0.018039831891655922, 0.0, 0.0, 0.0, 0.0], [0.03914211317896843, 0.005123047158122063, 0.07309678941965103, 0.008771137334406376, 0.006956204306334257, 0.0, 0.0, 0.0], [0.03503614291548729, 0.002023562788963318, 0.11927572637796402, 0.025566430762410164, 0.014959022402763367, 0.006848196964710951, 0.0, 0.0], [0.038021303713321686, 0.0009915122063830495, 0.028438609093427658, 0.019417235627770424, 0.007451367564499378, 0.0017408161656931043, 0.040869131684303284, 0.0], [0.03396092355251312, 0.005305602680891752, 0.12022184580564499, 0.011822246946394444, 0.0048513892106711864, 0.002342555206269026, 0.020128145813941956, 0.011446956545114517]]],\n","        };\n","        let labels = {\n","            \"set1\": ['L9H6', 'L9H9', 'L10H0'],\n","            \"set2\": ['L9H6', 'L9H9', 'L10H0'],\n","            \"set3\": ['L9H6', 'L9H9', 'L10H0'],\n","        };\n","\n","        window.changeTokens = function(value) {\n","            render(\"circuits-vis-7252ed2c-7408\", AttentionPatterns, {\"tokens\": tokens[value], \"attention\": attention[value], \"headLabels\": labels[value]});\n","        }\n","\n","        // Render the initial visualization\n","        changeTokens(\"set1\");\n","    </script>\n","\n","</body>\n","</html>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    attention_type = \"info-weighted\",\n","    radioitems = True,\n","    batch_labels = lambda str_toks: \"<code>\" + \"|\".join(str_toks) + \"</code>\", # These batch labels show each token separately\n","    head_notation = \"LH\", # head names are 10.7 rather than L10H7\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Attribution plots (work-in-progress!)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Attention plots might also be decent for logit attribution. The values of each cell are \"logits directly written in the correct direction\".\n","\n","You can pass the `resid_directions` argument to the function, and it'll measure the attribution in that direction (e.g. this could be a `logit_diff` vector). Note that it also supports `resid_directions` being a vector of length `d_vocab`; then it can convert this into a vector in the residual stream by mapping it backwards through `W_U` (and this also means we can do logit attribution for the unembedding bias `b_U`).\n","\n","There are 2 really hacky things about this function, which are why I don't recommend people use it yet.\n","\n","1. The colors are awkward; lots of translations and scalings have to be done to make sure that (1) the whitepoint is zero logit attribution and (2) no values are more than 1. The way I did this is by dividing all the component logit attributions by the maximum absolute value of all components' contributions over all sequence positions (which leads to a sparser & cleaner plot, and also makes it easier to make relative comparisons between different values). For the large plot `attention_heads`, this was sufficient, because it also supports negative values: `[-1, 0]` is red and `[0, 1]` is blue. For the smaller plot `attention_patterns`, there's only one color shade for each facet plot, and so this one has to be split up into 2 separate plots.\n","\n","2. If you're just doing attribution at a particular sequence position, it makes sense to just have a `(1, seq_len)`-shape plot per component, rather than a `(seq_len, seq_len)`-size plot where you only care about one row. Currently, circuitsvis doesn't support having different source and destination tokens so this isn't yet possible. A hacky solution: whenever `resid_directions` is just a single vector (i.e. it doesn't have a `seq_len` dimension), I broadcast it along the square attention plots, so you see a vertical stripe rather than just a dot. Uncomment the lines below to see this in action.\n","\n","I expect both of these two things to be solved eventually, but they're not high on my priority list right now."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["resid_directions = t.zeros(gpt2.cfg.d_vocab, dtype=t.float32, device=gpt2.cfg.device)\n","resid_directions[gpt2.to_single_token(\" John\")] = -1\n","resid_directions[gpt2.to_single_token(\" Mary\")] = +1\n","\n","# end_token_position = len(tokens) - 1 - tokens[::-1].index(\" to\")\n","\n","html_pos, html_neg = cv.attribution.from_cache(\n","    model = gpt2,\n","    cache = cache_full,\n","    tokens = tokens,\n","    mode = \"small\",\n","    layers = [-3, -2, -1],\n","    # seq_pos = end_token_position,\n","    # resid_directions = resid_directions,\n",")\n","display(html_pos)\n","display(html_neg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["html_all = cv.attribution.from_cache(\n","    model = gpt2,\n","    cache = cache_full,\n","    tokens = tokens,\n","    layers = [-3, -2, -1],\n","    mode = \"large\",\n",")\n","display(html_all)"]}],"metadata":{"colab":{"collapsed_sections":["_cQSNp3KZ5w6","wR1Wx-DmZ5w-","gNmjzhjEZ5w_","L4qafzzLjvYW","OWAlW_14Z5xA","4g7LcILIjpHG","2OZzBEuxjnBz","TBGtYCkgjlC5"],"provenance":[]},"kernelspec":{"display_name":"tl_intro_test","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
